/**
 * This file is part of Photo-SLAM
 *
 * Copyright (C) 2023-2024 Longwei Li and Hui Cheng, Sun Yat-sen University.
 * Copyright (C) 2023-2024 Huajian Huang and Sai-Kit Yeung, Hong Kong University
 * of Science and Technology.
 *
 * Photo-SLAM is free software: you can redistribute it and/or modify it under
 * the terms of the GNU General Public License as published by the Free Software
 * Foundation, either version 3 of the License, or (at your option) any later
 * version.
 *
 * Photo-SLAM is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
 * A PARTICULAR PURPOSE. See the GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License along with
 * Photo-SLAM. If not, see <http://www.gnu.org/licenses/>.
 */

#include <librealsense2/rsutil.h>
#include <torch/torch.h>

#include <algorithm>
#include <chrono>
#include <condition_variable>
#include <ctime>
#include <filesystem>
#include <fstream>
#include <iostream>
#include <librealsense2/rs.hpp>
#include <memory>
#include <opencv2/core/core.hpp>
#include <sstream>
#include <thread>

#include "ORB-SLAM3/include/System.h"
#include "include/gaussian_mapper.h"
#include "viewer/imgui_viewer.h"

rs2_stream find_stream_to_align(
    const std::vector<rs2::stream_profile>& streams) {
  // Given a vector of streams, we try to find a depth stream and another stream
  // to align depth with. We prioritize color streams to make the view look
  // better. If color is not available, we take another stream that (other than
  // depth)
  rs2_stream align_to = RS2_STREAM_ANY;
  bool depth_stream_found = false;
  bool color_stream_found = false;
  for (rs2::stream_profile sp : streams) {
    rs2_stream profile_stream = sp.stream_type();
    if (profile_stream != RS2_STREAM_DEPTH) {
      if (!color_stream_found)  // Prefer color
        align_to = profile_stream;

      if (profile_stream == RS2_STREAM_COLOR) {
        color_stream_found = true;
      }
    } else {
      depth_stream_found = true;
    }
  }

  if (!depth_stream_found)
    throw std::runtime_error("No Depth stream available");

  if (align_to == RS2_STREAM_ANY)
    throw std::runtime_error("No stream found to align with Depth");

  return align_to;
}

bool profile_changed(const std::vector<rs2::stream_profile>& current,
                     const std::vector<rs2::stream_profile>& prev) {
  for (auto&& sp : prev) {
    // If previous profile is in current (maybe just added another)
    auto itr = std::find_if(std::begin(current), std::end(current),
                            [&sp](const rs2::stream_profile& current_sp) {
                              return sp.unique_id() == current_sp.unique_id();
                            });
    if (itr ==
        std::end(current)) {  // If it previous stream wasn't found in current
      return true;
    }
  }
  return false;
}

static rs2_option get_sensor_option(const rs2::sensor& sensor) {
  // Sensors usually have several options to control their properties
  //  such as Exposure, Brightness etc.

  std::cout << "Sensor supports the following options:\n" << std::endl;

  // The following loop shows how to iterate over all available options
  // Starting from 0 until RS2_OPTION_COUNT (exclusive)
  for (int i = 0; i < static_cast<int>(RS2_OPTION_COUNT); i++) {
    rs2_option option_type = static_cast<rs2_option>(i);
    // SDK enum types can be streamed to get a string that represents them
    std::cout << "  " << i << ": " << option_type;

    // To control an option, use the following api:

    // First, verify that the sensor actually supports this option
    if (sensor.supports(option_type)) {
      std::cout << std::endl;

      // Get a human readable description of the option
      const char* description = sensor.get_option_description(option_type);
      std::cout << "       Description   : " << description << std::endl;

      // Get the current value of the option
      float current_value = sensor.get_option(option_type);
      std::cout << "       Current Value : " << current_value << std::endl;

      // To change the value of an option, please follow the
      // change_sensor_option() function
    } else {
      std::cout << " is not supported" << std::endl;
    }
  }

  uint32_t selected_sensor_option = 0;
  return static_cast<rs2_option>(selected_sensor_option);
}

void saveTrackingTime(std::vector<float>& vTimesTrack,
                      const std::string& strSavePath);
void saveGpuPeakMemoryUsage(std::filesystem::path pathSave);

int main(int argc, char** argv) {
  if (argc != 5) {
    std::cerr << std::endl
              << "Usage: " << argv[0] << " path_to_vocabulary" /*1*/
              << " path_to_ORB_SLAM3_settings"                 /*2*/
              << " path_to_gaussian_mapping_settings"          /*3*/
              << " path_to_output_directory/"                  /*4*/
              << std::endl;
    return 1;
  }
  bool use_viewer = true;

  std::string output_directory = std::string(argv[4]);
  if (output_directory.back() != '/') output_directory += "/";
  std::filesystem::path output_dir(output_directory);

  rs2::context ctx;
  rs2::device_list devices = ctx.query_devices();
  rs2::device selected_device;
  if (devices.size() == 0) {
    throw std::runtime_error(
        "No device connected, please connect a RealSense device.");
    return 0;
  } else {
    selected_device = devices[0];
  }

  std::vector<rs2::sensor> sensors = selected_device.query_sensors();
  int index = 0;
  // We can now iterate the sensors and print their names
  for (rs2::sensor sensor : sensors)
    if (sensor.supports(RS2_CAMERA_INFO_NAME)) {
      ++index;
      if (index == 1) {  // irSensor
        sensor.set_option(RS2_OPTION_ENABLE_AUTO_EXPOSURE, 1);
        sensor.set_option(RS2_OPTION_EMITTER_ENABLED,
                          1);  // emitter on for depth information
      }
      get_sensor_option(sensor);
      if (index == 2) {  // RGB camera
        // sensor.set_option(RS2_OPTION_ENABLE_AUTO_EXPOSURE, 0);
        sensor.set_option(RS2_OPTION_EXPOSURE, 150);
        sensor.set_option(RS2_OPTION_GAIN, 64);
      }

      if (index == 3) sensor.set_option(RS2_OPTION_ENABLE_MOTION_CORRECTION, 0);
    }
  // Declare RealSense pipeline, encapsulating the actual device and sensors
  rs2::pipeline pipe;

  // Create a configuration for configuring the pipeline with a non default
  // profile
  rs2::config cfg;

  // RGB stream
  cfg.enable_stream(RS2_STREAM_COLOR, 1280, 720, RS2_FORMAT_RGB8, 30);

  // Depth stream
  cfg.enable_stream(RS2_STREAM_DEPTH, 1280, 720, RS2_FORMAT_Z16, 30);

  // IMU stream
  cfg.enable_stream(RS2_STREAM_ACCEL, RS2_FORMAT_MOTION_XYZ32F);
  cfg.enable_stream(RS2_STREAM_GYRO, RS2_FORMAT_MOTION_XYZ32F);

  // IMU callback
  std::mutex imu_mutex;
  std::condition_variable cond_image_rec;

  std::vector<double> v_gyro_timestamp;

  rs2_vector current_accel_data;
  std::vector<double> v_accel_timestamp_sync;
  std::vector<rs2_vector> v_accel_data_sync;

  cv::Mat imCV, depthCV;
  int width_img, height_img;
  double timestamp_image = -1.0;
  bool image_ready = false;
  int count_im_buffer = 0;  // count dropped frames

  // start and stop just to get necessary profile
  rs2::pipeline_profile pipe_profile = pipe.start(cfg);
  pipe.stop();

  // Align depth and RGB frames
  //  Pipeline could choose a device that does not have a color stream
  //  If there is no color stream, choose to align depth to another stream
  rs2_stream align_to = find_stream_to_align(pipe_profile.get_streams());

  // Create a rs2::align object.
  //  rs2::align allows us to perform alignment of depth frames to others frames
  //  The "align_to" is the stream type to which we plan to align depth frames.
  rs2::align align(align_to);
  rs2::frameset fsSLAM;

  auto imu_callback = [&](const rs2::frame& frame) {
    std::unique_lock<std::mutex> lock(imu_mutex);

    if (rs2::frameset fs = frame.as<rs2::frameset>()) {
      count_im_buffer++;

      double new_timestamp_image = fs.get_timestamp() * 1e-3;
      if (abs(timestamp_image - new_timestamp_image) < 0.001) {
        count_im_buffer--;
        return;
      }

      if (profile_changed(pipe.get_active_profile().get_streams(),
                          pipe_profile.get_streams())) {
        // If the profile was changed, update the align object, and also get the
        // new device's depth scale
        pipe_profile = pipe.get_active_profile();
        align_to = find_stream_to_align(pipe_profile.get_streams());
        align = rs2::align(align_to);
      }

      // Align depth and rgb takes long time, move it out of the interruption to
      // avoid losing IMU measurements
      fsSLAM = fs;

      timestamp_image = fs.get_timestamp() * 1e-3;
      image_ready = true;

      while (v_gyro_timestamp.size() > v_accel_timestamp_sync.size()) {
        int index = v_accel_timestamp_sync.size();
        double target_time = v_gyro_timestamp[index];

        v_accel_data_sync.push_back(current_accel_data);
        v_accel_timestamp_sync.push_back(target_time);
      }

      lock.unlock();
      cond_image_rec.notify_all();
    }
  };

  pipe_profile = pipe.start(cfg, imu_callback);
  std::cout << 111 << std::endl;
  rs2::stream_profile cam_stream = pipe_profile.get_stream(RS2_STREAM_COLOR);
  rs2_intrinsics intrinsics_cam =
      cam_stream.as<rs2::video_stream_profile>().get_intrinsics();
  width_img = intrinsics_cam.width;
  height_img = intrinsics_cam.height;
  std::cout << " fx = " << intrinsics_cam.fx << std::endl;
  std::cout << " fy = " << intrinsics_cam.fy << std::endl;
  std::cout << " cx = " << intrinsics_cam.ppx << std::endl;
  std::cout << " cy = " << intrinsics_cam.ppy << std::endl;
  std::cout << " height = " << intrinsics_cam.height << std::endl;
  std::cout << " width = " << intrinsics_cam.width << std::endl;
  std::cout << " Coeff = " << intrinsics_cam.coeffs[0] << ", "
            << intrinsics_cam.coeffs[1] << ", " << intrinsics_cam.coeffs[2]
            << ", " << intrinsics_cam.coeffs[3] << ", "
            << intrinsics_cam.coeffs[4] << ", " << std::endl;
  std::cout << " Model = " << intrinsics_cam.model << std::endl;

  // Device
  torch::DeviceType device_type;
  if (torch::cuda::is_available()) {
    std::cout << "CUDA available! Training on GPU." << std::endl;
    device_type = torch::kCUDA;
  } else {
    std::cout << "Training on CPU." << std::endl;
    device_type = torch::kCPU;
  }

  // Create SLAM system. It initializes all system threads and gets ready to
  // process frames.
  std::shared_ptr<ORB_SLAM3::System> pSLAM =
      std::make_shared<ORB_SLAM3::System>(
          argv[1], argv[2], ORB_SLAM3::System::RGBD, 0, "realsense_rgbd");
  float imageScale = pSLAM->GetImageScale();

  // Create GaussianMapper
  std::filesystem::path gaussian_cfg_path(argv[3]);
  std::shared_ptr<GaussianMapper> pGausMapper =
      std::make_shared<GaussianMapper>(pSLAM, gaussian_cfg_path, output_dir, 0,
                                       device_type);
  std::thread training_thd(&GaussianMapper::run, pGausMapper.get());

  // Create Gaussian Viewer
  std::thread viewer_thd;
  std::shared_ptr<ImGuiViewer> pViewer;
  if (use_viewer) {
    pViewer = std::make_shared<ImGuiViewer>(pSLAM, pGausMapper);
    viewer_thd = std::thread(&ImGuiViewer::run, pViewer.get());
  }

  // Vector for tracking time statistics
  std::vector<float> vTimesTrack;
  vTimesTrack.reserve(3000000);

  double timestamp;
  cv::Mat im, depth;

  rs2::frameset fs;

  while (!pSLAM->isShutDown()) {
    {
      std::unique_lock<std::mutex> lk(imu_mutex);
      if (!image_ready) cond_image_rec.wait(lk);

      fs = fsSLAM;

      if (count_im_buffer > 1) cout << count_im_buffer - 1 << " dropped frs\n";
      count_im_buffer = 0;

      timestamp = timestamp_image;
      im = imCV.clone();
      depth = depthCV.clone();

      image_ready = false;
    }

    // Perform alignment here
    auto processed = align.process(fs);

    // Trying to get both other and aligned depth frames
    rs2::video_frame color_frame = processed.first(align_to);
    rs2::depth_frame depth_frame = processed.get_depth_frame();

    im = cv::Mat(cv::Size(width_img, height_img), CV_8UC3,
                 (void*)(color_frame.get_data()), cv::Mat::AUTO_STEP);
    depth = cv::Mat(cv::Size(width_img, height_img), CV_16U,
                    (void*)(depth_frame.get_data()), cv::Mat::AUTO_STEP);

    if (imageScale != 1.f) {
      int width = im.cols * imageScale;
      int height = im.rows * imageScale;
      cv::resize(im, im, cv::Size(width, height));
      cv::resize(depth, depth, cv::Size(width, height));
    }

    std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();

    // Pass the image to the SLAM system
    pSLAM->TrackRGBD(im, depth, timestamp, std::vector<ORB_SLAM3::IMU::Point>(),
                     std::to_string(timestamp));

    std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
    double ttrack =
        std::chrono::duration_cast<std::chrono::duration<double>>(t2 - t1)
            .count();
    vTimesTrack.emplace_back(ttrack);
  }

  // Stop all threads
  pSLAM->Shutdown();
  training_thd.join();
  if (use_viewer) viewer_thd.join();

  // GPU peak usage
  saveGpuPeakMemoryUsage(output_dir / "GpuPeakUsageMB.txt");

  // Tracking time statistics
  saveTrackingTime(vTimesTrack, (output_dir / "TrackingTime.txt").string());

  // Save camera trajectory
  pSLAM->SaveTrajectoryTUM((output_dir / "CameraTrajectory_TUM.txt").string());
  pSLAM->SaveKeyFrameTrajectoryTUM(
      (output_dir / "KeyFrameTrajectory_TUM.txt").string());
  pSLAM->SaveTrajectoryEuRoC(
      (output_dir / "CameraTrajectory_EuRoC.txt").string());
  pSLAM->SaveKeyFrameTrajectoryEuRoC(
      (output_dir / "KeyFrameTrajectory_EuRoC.txt").string());
  pSLAM->SaveTrajectoryKITTI(
      (output_dir / "CameraTrajectory_KITTI.txt").string());

  return 0;
}

void saveTrackingTime(std::vector<float>& vTimesTrack,
                      const std::string& strSavePath) {
  std::ofstream out;
  out.open(strSavePath.c_str());
  std::size_t nImages = vTimesTrack.size();
  float totaltime = 0;
  for (int ni = 0; ni < nImages; ni++) {
    out << std::fixed << std::setprecision(4) << vTimesTrack[ni] << std::endl;
    totaltime += vTimesTrack[ni];
  }

  // std::sort(vTimesTrack.begin(), vTimesTrack.end());
  // out << "-------" << std::endl;
  // out << std::fixed << std::setprecision(4)
  //     << "median tracking time: " << vTimesTrack[nImages / 2] << std::endl;
  // out << std::fixed << std::setprecision(4)
  //     << "mean tracking time: " << totaltime / nImages << std::endl;

  out.close();
}

void saveGpuPeakMemoryUsage(std::filesystem::path pathSave) {
  namespace c10Alloc = c10::cuda::CUDACachingAllocator;
  c10Alloc::DeviceStats mem_stats = c10Alloc::getDeviceStats(0);

  c10Alloc::Stat reserved_bytes =
      mem_stats.reserved_bytes[static_cast<int>(c10Alloc::StatType::AGGREGATE)];
  float max_reserved_MB = reserved_bytes.peak / (1024.0 * 1024.0);

  c10Alloc::Stat alloc_bytes =
      mem_stats
          .allocated_bytes[static_cast<int>(c10Alloc::StatType::AGGREGATE)];
  float max_alloc_MB = alloc_bytes.peak / (1024.0 * 1024.0);

  std::ofstream out(pathSave);
  out << "Peak reserved (MB): " << max_reserved_MB << std::endl;
  out << "Peak allocated (MB): " << max_alloc_MB << std::endl;
  out.close();
}
